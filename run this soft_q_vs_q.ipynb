{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ec7296",
   "metadata": {},
   "source": [
    "# Soft-Q-Routing vs Q-Routing Training Comparison in IAB Networks\n",
    "\n",
    "This notebook demonstrates and compares the training behavior of two reinforcement learning routing algorithms in Integrated Access Backhaul (IAB) networks:\n",
    "\n",
    "##  Experiment Objectives\n",
    "- **Compare Training Convergence**: Analyze how quickly each algorithm learns optimal routing policies\n",
    "- **Training Stability**: Observe the stability of learning curves during training\n",
    "- **Algorithm Behavior**: Understand the differences in how each algorithm explores and exploits the network\n",
    "\n",
    "##  Algorithm Overview\n",
    "\n",
    "### Q-Routing (Traditional)\n",
    "- **Deterministic Policy**: Uses max Q-value for action selection (greedy with ε-exploration)\n",
    "- **Hard Decisions**: Makes crisp routing decisions based on highest Q-values\n",
    "- **Exploration**: Relies on ε-greedy exploration strategy\n",
    "- **Update Rule**: Standard Q-learning update with temporal difference\n",
    "\n",
    "### Soft-Q-Routing (Energy-Based)\n",
    "- **Probabilistic Policy**: Uses Boltzmann/softmax distribution for action selection\n",
    "- **Soft Decisions**: Considers all possible actions weighted by their Q-values\n",
    "- **Temperature Parameter**: Controls exploration vs exploitation trade-off\n",
    "- **Robust Learning**: More resilient to local optima and network changes\n",
    "\n",
    "##  What We'll Analyze\n",
    "1. **Training Progress**: Reward accumulation over episodes\n",
    "2. **Learning Curves**: How quickly each algorithm improves\n",
    "3. **Convergence Patterns**: Stability and final performance\n",
    "4. **Action Selection**: Differences in routing decision patterns\n",
    "\n",
    "##  Experiment Setup\n",
    "- **Training Only**: Focus on learning behavior without evaluation overhead\n",
    "- **Same Environment**: Both algorithms train on identical network conditions\n",
    "- **Controlled Comparison**: Same hyperparameters where applicable\n",
    "- **Visualization**: Real-time training progress and final comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14ad57",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "We'll start by installing all necessary packages for our experiment. This includes:\n",
    "- **Core ML Libraries**: NumPy, pandas for data handling\n",
    "- **Visualization**: Matplotlib, seaborn for plotting training progress\n",
    "- **Deep Learning**: PyTorch for neural network components (if used)\n",
    "- **Project Modules**: Custom IAB network simulation environment and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for the IAB routing experiment\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "# Define required packages for our routing experiment\n",
    "required_packages = [\n",
    "    'bunch==1.0.1',         # Data structure utilities\n",
    "    'gym==0.26.2',          # Reinforcement learning environment framework\n",
    "    'matplotlib==3.5.1',    # Plotting and visualization\n",
    "    'networkx==2.7.1',      # Network topology analysis\n",
    "    'numpy==1.21.5',        # Numerical computing\n",
    "    'pandas==1.4.2',        # Data analysis and manipulation\n",
    "    'scipy==1.7.3',         # Scientific computing\n",
    "    'seaborn==0.11.2',      # Statistical data visualization\n",
    "    'torch==1.13.0',        # Deep learning framework\n",
    "    'tqdm==4.64.0',         # Progress bars for training loops\n",
    "    'jupyter',              # Jupyter notebook support\n",
    "    'ipykernel',            # IPython kernel for notebooks\n",
    "    'ipywidgets'            # Interactive widgets for notebooks\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a84a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries for our routing experiment\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Configure matplotlib for beautiful inline plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Importing project-specific modules...\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Import custom IAB network simulation modules\n",
    "    from Utils import Statistics                    # Performance metrics collection\n",
    "    from Utils.ml_flow_utils import preprocess_meta_data  # Configuration loading\n",
    "    from Environment import env                     # Network environment simulation\n",
    "    from Agents import Agent                        # Q-learning and Soft-Q agents\n",
    "    from trainer import trainer, evaluator         # Training and evaluation frameworks\n",
    "    \n",
    "    print(\"All project modules imported successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules: {e}\")\n",
    "    print(\"Make sure you're running this notebook from the project root directory.\")\n",
    "    \n",
    "# Setup experiment logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"soft_q_vs_q_experiment\")\n",
    "\n",
    "print(\"\\nSystem Information:\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "print(\"\\n Ready to start the Soft-Q vs Q-Routing experiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a7468",
   "metadata": {},
   "source": [
    "## 2. Load and Configure Experiment Settings\n",
    "\n",
    "Here we'll load the network configuration and set up our experiment parameters. We'll focus on training-only parameters since we're primarily interested in comparing the learning behavior of the two algorithms.\n",
    "\n",
    "### Configuration Components:\n",
    "- **Network Topology**: Number of base stations, users, and network capacity\n",
    "- **Training Parameters**: Episodes, time steps, learning rates\n",
    "- **Algorithm Settings**: Exploration parameters, update frequencies\n",
    "- **Logging**: Where to save training progress and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458052ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and configure experiment settings\n",
    "print(\" Loading experiment configuration...\")\n",
    "\n",
    "try:\n",
    "    # Try to load existing configuration\n",
    "    setting, args, temp_device, experiment = preprocess_meta_data()\n",
    "    print(\" Configuration loaded from existing settings!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Could not load existing configuration: {e}\")\n",
    "    print(\" Creating default configuration for training experiment...\")\n",
    "    \n",
    "    # Create a comprehensive configuration for our training experiment\n",
    "    setting = {\n",
    "        \"NETWORK\": {\n",
    "            \"holding capacity\": 10,      # Buffer size at each node\n",
    "            \"number Basestation\": 6,     # Number of base stations in IAB network\n",
    "            \"number user\": 12            # Number of user equipment (UE) nodes\n",
    "        },\n",
    "        \"Simulation\": {\n",
    "            # Training-focused parameters (no test settings needed)\n",
    "            \"training_episodes\": 50,     # Number of training episodes\n",
    "            \"max_allowed_time_step_per_episode\": 2000,  # Steps per episode\n",
    "            \"num_time_step_to_update_target_network\": 200,  # Target network update frequency\n",
    "        },\n",
    "        \"AGENT\": {\n",
    "            \"enable_train\": True,        # Enable training mode\n",
    "            \"checkpoint_frequency\": 10,  # Save model every N episodes\n",
    "            \"learning_freq\": 1,          # Learning frequency (every step)\n",
    "            \"rewardfunction\": \"latency_throughput\",  # Reward function type\n",
    "            \"learning_rate\": 0.001,      # Learning rate for both algorithms\n",
    "            \"epsilon\": 0.1,              # Exploration rate for Q-learning\n",
    "            \"epsilon_decay\": 0.995,      # Epsilon decay rate\n",
    "            \"temperature\": 1.0,          # Temperature for Soft-Q (controls exploration)\n",
    "            \"temperature_decay\": 0.99    # Temperature decay rate\n",
    "        },\n",
    "        \"seed\": 42,                      # Random seed for reproducibility\n",
    "        \"result_dir\": \"./training_results\"  # Directory to save training results\n",
    "    }\n",
    "    experiment = None\n",
    "\n",
    "# Configure derived network parameters\n",
    "setting[\"capacity\"] = setting[\"NETWORK\"][\"holding capacity\"]\n",
    "setting[\"num_nodes\"] = setting[\"NETWORK\"][\"number Basestation\"] + setting[\"NETWORK\"][\"number user\"]\n",
    "setting[\"num_bs\"] = setting[\"NETWORK\"][\"number Basestation\"]\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(setting[\"result_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\" Network Configuration:\")\n",
    "print(f\"   • Base Stations: {setting['num_bs']}\")\n",
    "print(f\"   • User Equipment: {setting['NETWORK']['number user']}\")\n",
    "print(f\"   • Total Nodes: {setting['num_nodes']}\")\n",
    "print(f\"   • Buffer Capacity: {setting['capacity']} packets per node\")\n",
    "\n",
    "print(f\"\\n Training Configuration:\")\n",
    "print(f\"   • Training Episodes: {setting['Simulation']['training_episodes']}\")\n",
    "print(f\"   • Time Steps per Episode: {setting['Simulation']['max_allowed_time_step_per_episode']}\")\n",
    "print(f\"   • Target Network Updates: Every {setting['Simulation']['num_time_step_to_update_target_network']} steps\")\n",
    "print(f\"   • Checkpoint Frequency: Every {setting['AGENT']['checkpoint_frequency']} episodes\")\n",
    "\n",
    "print(f\"\\n Algorithm Parameters:\")\n",
    "print(f\"   • Learning Rate: {setting['AGENT']['learning_rate']}\")\n",
    "print(f\"   • Q-Learning Epsilon: {setting['AGENT']['epsilon']} (decay: {setting['AGENT']['epsilon_decay']})\")\n",
    "print(f\"   • Soft-Q Temperature: {setting['AGENT']['temperature']} (decay: {setting['AGENT']['temperature_decay']})\")\n",
    "print(f\"   • Reward Function: {setting['AGENT']['rewardfunction']}\")\n",
    "\n",
    "print(f\"\\n Output:\")\n",
    "print(f\"   • Results Directory: {setting['result_dir']}\")\n",
    "print(f\"   • Random Seed: {setting['seed']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n Configuration ready for training experiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae2380",
   "metadata": {},
   "source": [
    "## 3. Initialize Environment and Agent Classes\n",
    "\n",
    "Now we'll set up the different components needed for our comparative training experiment:\n",
    "\n",
    "### Environment Setup\n",
    "- **Same Environment**: Both algorithms will train on identical network topologies\n",
    "- **Dynamic Network**: The IAB network changes over time to test adaptability\n",
    "- **Packet Routing**: Agents learn to route packets efficiently through the network\n",
    "\n",
    "### Agent Differences\n",
    "- **Q-Agent**: Traditional Q-learning with ε-greedy exploration\n",
    "- **SoftQ-Agent**: Soft Q-learning with Boltzmann exploration policy\n",
    "\n",
    "### Why This Comparison Matters\n",
    "- **Exploration Strategy**: Different approaches to exploring unknown network states\n",
    "- **Action Selection**: Hard vs soft decision making in routing\n",
    "- **Convergence**: How stable and fast each algorithm learns optimal policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment and agent lookup tables for our experiment\n",
    "print(\" Setting up environment and agent configurations...\")\n",
    "\n",
    "# Environment Classes - Both algorithms use the same Q-learning environment\n",
    "# This ensures fair comparison since the underlying network dynamics are identical\n",
    "envsLut = {\n",
    "    'Q-Routing': env.dynetworkEnvQlearning,      # Standard Q-learning environment\n",
    "    'Soft-Q-Routing': env.dynetworkEnvQlearning,  # Same environment for Soft-Q\n",
    "}\n",
    "\n",
    "# Agent Classes - This is where the key difference lies\n",
    "agentsLut = {\n",
    "    'Q-Routing': Agent.QAgent,        # Traditional Q-learning agent (ε-greedy)\n",
    "    'Soft-Q-Routing': Agent.SoftQAgent,  # Soft Q-learning agent (Boltzmann policy)\n",
    "}\n",
    "\n",
    "# Trainer Classes - Both use the same trainer framework\n",
    "trainerLut = {\n",
    "    'Q-Routing': trainer.RLTabularTrainer,      # Standard RL trainer\n",
    "    'Soft-Q-Routing': trainer.RLTabularTrainer,  # Same trainer for fair comparison\n",
    "}\n",
    "\n",
    "# Since we're focusing on training only, we'll simplify the evaluator setup\n",
    "# We'll use basic statistics collection instead of full evaluation\n",
    "evaluatorLut = {\n",
    "    'Q-Routing': None,        # No evaluation, training metrics only\n",
    "    'Soft-Q-Routing': None,   # No evaluation, training metrics only\n",
    "}\n",
    "\n",
    "# Define the algorithms we'll compare\n",
    "agent_names = ['Q-Routing', 'Soft-Q-Routing']\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" EXPERIMENT SETUP SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, name in enumerate(agent_names, 1):\n",
    "    env_class = envsLut[name]\n",
    "    agent_class = agentsLut[name]\n",
    "    trainer_class = trainerLut[name]\n",
    "    \n",
    "    print(f\"\\n{i}. {name}:\")\n",
    "    print(f\"    Environment: {env_class.__name__}\")\n",
    "    print(f\"    Agent Type: {agent_class.__name__}\")\n",
    "    print(f\"    Trainer: {trainer_class.__name__}\")\n",
    "    \n",
    "    # Explain the key differences\n",
    "    if name == 'Q-Routing':\n",
    "        print(f\"    Policy: ε-greedy (deterministic with random exploration)\")\n",
    "        print(f\"    Action Selection: Choose action with highest Q-value\")\n",
    "    else:\n",
    "        print(f\"    Policy: Boltzmann/Softmax (probabilistic)\")\n",
    "        print(f\"    Action Selection: Sample from Q-value probability distribution\")\n",
    "\n",
    "print(f\"\\n Comparison Focus: Training behavior of {' vs '.join(agent_names)}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n Environment and agent classes configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef4e523",
   "metadata": {},
   "source": [
    "## 4. Create Training Experiment Class\n",
    "\n",
    "We'll create a simplified experiment class focused entirely on training comparison. This class will:\n",
    "\n",
    "- **Initialize Agents**: Set up both Q-learning and Soft-Q agents with identical network conditions\n",
    "- **Training Infrastructure**: Prepare trainers and statistics collectors for both algorithms\n",
    "- **Progress Tracking**: Monitor training progress in real-time\n",
    "- **Fair Comparison**: Ensure both algorithms have identical starting conditions and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bdcbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified training-focused experiment class\n",
    "class TrainingExperiment:\n",
    "    \"\"\"\n",
    "    Simplified experiment class focused on comparing training behavior\n",
    "    of Q-Routing vs Soft-Q-Routing algorithms\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, setting, experiment, agent_names):\n",
    "        \"\"\"\n",
    "        Initialize the training experiment\n",
    "        \n",
    "        Args:\n",
    "            setting: Configuration dictionary\n",
    "            experiment: Experiment metadata (can be None)\n",
    "            agent_names: List of algorithm names to compare\n",
    "        \"\"\"\n",
    "        print(\" Initializing Training Experiment...\")\n",
    "        \n",
    "        self.setting = setting\n",
    "        self.experiment = experiment\n",
    "        self.agent_names = agent_names\n",
    "        \n",
    "        # Storage for experiment components\n",
    "        self.agents = {}      # Agent instances\n",
    "        self.envs = {}        # Environment instances  \n",
    "        self.trainers = {}    # Trainer instances\n",
    "        self.stats = {}       # Statistics collectors\n",
    "        self.paths = {}       # Result directories\n",
    "        self.training_metrics = {}  # Training progress tracking\n",
    "        \n",
    "        self._initialize_components()\n",
    "        \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize all components for each algorithm\"\"\"\n",
    "        \n",
    "        for name in self.agent_names:\n",
    "            print(f\"\\n Setting up {name}...\")\n",
    "            \n",
    "            # Create result directories\n",
    "            self.paths[name] = os.path.join(self.setting[\"result_dir\"], name)\n",
    "            os.makedirs(self.paths[name], exist_ok=True)\n",
    "            \n",
    "            # Get component classes from lookup tables\n",
    "            EnvClass = envsLut[name]\n",
    "            AgentClass = agentsLut[name]\n",
    "            TrainerClass = trainerLut[name]\n",
    "            \n",
    "            print(f\"    Creating environment: {EnvClass.__name__}\")\n",
    "            # Initialize environment with consistent settings\n",
    "            env_instance = EnvClass(\n",
    "                setting=self.setting, \n",
    "                seed=self.setting[\"seed\"], \n",
    "                algorithm=name, \n",
    "                rewardfun=self.setting[\"AGENT\"][\"rewardfunction\"]\n",
    "            )\n",
    "            \n",
    "            print(f\"    Creating agent: {AgentClass.__name__}\")\n",
    "            # Get state space dimensions and create agent\n",
    "            state_space = env_instance.get_state_space_dim(self.setting)\n",
    "            agent_instance = AgentClass(env_instance.dynetwork, self.setting, state_space, None)\n",
    "            \n",
    "            # Store instances\n",
    "            self.envs[name] = env_instance\n",
    "            self.agents[name] = agent_instance\n",
    "            \n",
    "            print(f\"    Setting up statistics collection...\")\n",
    "            # Initialize training statistics collector\n",
    "            self.stats[name] = Statistics.TrainQLStatisticsCollector(\n",
    "                setting=self.setting, \n",
    "                result_dir=self.paths[name], \n",
    "                algorithms=[name]\n",
    "            )\n",
    "            \n",
    "            print(f\"    Creating trainer: {TrainerClass.__name__}\")\n",
    "            # Initialize trainer for this algorithm\n",
    "            self.trainers[name] = TrainerClass(\n",
    "                time_steps=self.setting[\"Simulation\"][\"max_allowed_time_step_per_episode\"],\n",
    "                TARGET_UPDATE=self.setting[\"Simulation\"][\"num_time_step_to_update_target_network\"],\n",
    "                agent=agent_instance,\n",
    "                stat_collector=self.stats[name],\n",
    "                env=env_instance,\n",
    "                name=name,\n",
    "                writer=self.setting.get(\"train_writer\", None),\n",
    "                experiment=self.experiment,\n",
    "                update_freq=self.setting[\"AGENT\"][\"learning_freq\"]\n",
    "            )\n",
    "            \n",
    "            # Initialize training metrics tracking\n",
    "            self.training_metrics[name] = {\n",
    "                'episode_rewards': [],\n",
    "                'episode_lengths': [],\n",
    "                'losses': [],\n",
    "                'exploration_rates': [],\n",
    "                'timestamps': []\n",
    "            }\n",
    "            \n",
    "            print(f\"    {name} setup complete!\")\n",
    "        \n",
    "        print(f\"\\n Training experiment ready with {len(self.agent_names)} algorithms!\")\n",
    "        print(f\" Results will be saved to: {self.setting['result_dir']}\")\n",
    "\n",
    "# Create the training experiment instance\n",
    "print(\"Creating training experiment instance...\")\n",
    "experiment_instance = TrainingExperiment(\n",
    "    setting=setting, \n",
    "    experiment=experiment, \n",
    "    agent_names=agent_names\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" EXPERIMENT INITIALIZATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\" Ready to compare: {' vs '.join(agent_names)}\")\n",
    "print(f\" Training episodes: {setting['Simulation']['training_episodes']}\")\n",
    "print(f\" Steps per episode: {setting['Simulation']['max_allowed_time_step_per_episode']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda660dc",
   "metadata": {},
   "source": [
    "## 5. Verify Training Setup\n",
    "\n",
    "Let's verify that our training components are properly configured and ready to run. We'll check:\n",
    "\n",
    "- **Agent Configuration**: Confirm both agents are properly initialized\n",
    "- **Training Parameters**: Verify learning rates, exploration settings\n",
    "- **Environment State**: Ensure both algorithms start with identical conditions\n",
    "- **Statistics Collection**: Confirm we can track training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae425af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all training components are properly set up\n",
    "print(\" Verifying training setup...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" TRAINING COMPONENT VERIFICATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "all_ready = True\n",
    "\n",
    "for i, name in enumerate(agent_names, 1):\n",
    "    print(f\"\\n{i}. {name} Configuration:\")\n",
    "    \n",
    "    # Check agent\n",
    "    agent = experiment_instance.agents[name]\n",
    "    print(f\"    Agent: {type(agent).__name__} \")\n",
    "    \n",
    "    # Check environment\n",
    "    env = experiment_instance.envs[name]\n",
    "    print(f\"    Environment: {type(env).__name__} \")\n",
    "    \n",
    "    # Check trainer\n",
    "    trainer = experiment_instance.trainers[name]\n",
    "    if trainer is not None:\n",
    "        print(f\"    Trainer: {type(trainer).__name__} \")\n",
    "    else:\n",
    "        print(f\"    Trainer: Missing \")\n",
    "        all_ready = False\n",
    "    \n",
    "    # Check statistics collector\n",
    "    stats = experiment_instance.stats[name]\n",
    "    print(f\"    Statistics: {type(stats).__name__} \")\n",
    "    \n",
    "    # Check result directory\n",
    "    path = experiment_instance.paths[name]\n",
    "    if os.path.exists(path):\n",
    "        print(f\"    Results Path: {path} \")\n",
    "    else:\n",
    "        print(f\"    Results Path: {path} \")\n",
    "        all_ready = False\n",
    "    \n",
    "    # Display algorithm-specific parameters\n",
    "    if name == 'Q-Routing':\n",
    "        print(f\"    Epsilon: {setting['AGENT']['epsilon']} (decay: {setting['AGENT']['epsilon_decay']})\")\n",
    "    else:  # Soft-Q-Routing\n",
    "        print(f\"    Temperature: {setting['AGENT']['temperature']} (decay: {setting['AGENT']['temperature_decay']})\")\n",
    "\n",
    "print(f\"\\n Shared Training Parameters:\")\n",
    "print(f\"   • Learning Rate: {setting['AGENT']['learning_rate']}\")\n",
    "print(f\"   • Training Episodes: {setting['Simulation']['training_episodes']}\")\n",
    "print(f\"   • Steps per Episode: {setting['Simulation']['max_allowed_time_step_per_episode']}\")\n",
    "print(f\"   • Checkpoint Frequency: {setting['AGENT']['checkpoint_frequency']} episodes\")\n",
    "print(f\"   • Target Network Update: Every {setting['Simulation']['num_time_step_to_update_target_network']} steps\")\n",
    "\n",
    "print(f\"\\n Network Environment:\")\n",
    "print(f\"   • Total Nodes: {setting['num_nodes']}\")\n",
    "print(f\"   • Base Stations: {setting['num_bs']}\")\n",
    "print(f\"   • User Equipment: {setting['NETWORK']['number user']}\")\n",
    "print(f\"   • Buffer Capacity: {setting['capacity']} packets/node\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "if all_ready:\n",
    "    print(\" ALL COMPONENTS READY FOR TRAINING!\")\n",
    "    print(\" Ready to start the comparative training experiment!\")\n",
    "else:\n",
    "    print(\" Some components are not properly configured.\")\n",
    "    print(\"Please check the error messages above before proceeding.\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9efba5",
   "metadata": {},
   "source": [
    "## 6. Run Training Experiment\n",
    "\n",
    "Now we'll run the main training experiment! This will:\n",
    "\n",
    "### Training Process\n",
    "1. **Initialize**: Both algorithms start with random policies\n",
    "2. **Learn**: Each algorithm experiences the same network episodes\n",
    "3. **Track Progress**: Monitor rewards, convergence, and learning curves\n",
    "4. **Save Checkpoints**: Periodically save model states\n",
    "\n",
    "### What to Expect\n",
    "- **Q-Routing**: Should show more aggressive exploration initially, then quick convergence\n",
    "- **Soft-Q-Routing**: Should show smoother exploration and potentially more stable convergence\n",
    "- **Progress Bars**: Real-time training progress for both algorithms\n",
    "- **Live Metrics**: Episode rewards and learning statistics\n",
    "\n",
    "### Training Duration\n",
    "This will take several minutes depending on your hardware. We'll see real-time progress as both algorithms learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d62432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training experiment with detailed progress tracking\n",
    "def run_training_experiment(experiment_instance):\n",
    "    \"\"\"\n",
    "    Run training for both algorithms and track their progress\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training results and metrics for both algorithms\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" Starting Soft-Q vs Q-Routing Training Experiment!\")\n",
    "    print(f\" Start time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = {}\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    # Train each algorithm\n",
    "    for algorithm_idx, name in enumerate(experiment_instance.agent_names):\n",
    "        print(f\"\\n{'Q' if name == 'Q-Routing' else 'SOftQ'} TRAINING {name.upper()}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Get components for this algorithm\n",
    "        trainer = experiment_instance.trainers[name]\n",
    "        agent = experiment_instance.agents[name]\n",
    "        metrics = experiment_instance.training_metrics[name]\n",
    "        \n",
    "        if trainer is None:\n",
    "            print(f\" No trainer available for {name}\")\n",
    "            results[name] = None\n",
    "            continue\n",
    "        \n",
    "        # Training parameters\n",
    "        total_episodes = experiment_instance.setting[\"Simulation\"][\"training_episodes\"]\n",
    "        checkpoint_freq = experiment_instance.setting[\"AGENT\"][\"checkpoint_frequency\"]\n",
    "        \n",
    "        print(f\" Episodes to train: {total_episodes}\")\n",
    "        print(f\" Checkpoint frequency: Every {checkpoint_freq} episodes\")\n",
    "        print(f\" Max steps per episode: {experiment_instance.setting['Simulation']['max_allowed_time_step_per_episode']}\")\n",
    "        \n",
    "        # Training loop with progress tracking\n",
    "        episode_rewards = []\n",
    "        algorithm_start_time = time.time()\n",
    "        \n",
    "        for episode in tqdm(range(total_episodes), \n",
    "                           desc=f\"Training {name}\", \n",
    "                           ncols=100, \n",
    "                           colour='blue' if name == 'Q-Routing' else 'green'):\n",
    "            \n",
    "            try:\n",
    "                # Record episode start time\n",
    "                episode_start_time = time.time()\n",
    "                \n",
    "                # Train one episode\n",
    "                trainer.train(episode)\n",
    "                \n",
    "                # Track episode metrics\n",
    "                episode_duration = time.time() - episode_start_time\n",
    "                \n",
    "                # Get episode reward (if available from trainer)\n",
    "                episode_reward = getattr(trainer, 'last_episode_reward', 0)\n",
    "                episode_rewards.append(episode_reward)\n",
    "                \n",
    "                # Store metrics\n",
    "                metrics['episode_rewards'].append(episode_reward)\n",
    "                metrics['episode_lengths'].append(episode_duration)\n",
    "                metrics['timestamps'].append(time.time())\n",
    "                \n",
    "                # Get current exploration parameter\n",
    "                if name == 'Q-Routing':\n",
    "                    exploration_param = getattr(agent, 'epsilon', setting['AGENT']['epsilon'])\n",
    "                else:  # Soft-Q-Routing\n",
    "                    exploration_param = getattr(agent, 'temperature', setting['AGENT']['temperature'])\n",
    "                metrics['exploration_rates'].append(exploration_param)\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if episode % checkpoint_freq == 0 and episode > 0:\n",
    "                    agent.save_agent(experiment_instance.paths[name])\n",
    "                    \n",
    "                    # Print progress update\n",
    "                    avg_reward = np.mean(episode_rewards[-checkpoint_freq:])\n",
    "                    print(f\"\\\\n Episode {episode}: Avg Reward = {avg_reward:.3f}, \"\n",
    "                          f\"Exploration = {exploration_param:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\\\n Error in episode {episode}: {e}\")\n",
    "                # Continue training despite errors\n",
    "                continue\n",
    "        \n",
    "        # Final save\n",
    "        agent.save_agent(experiment_instance.paths[name])\n",
    "        algorithm_duration = time.time() - algorithm_start_time\n",
    "        \n",
    "        # Calculate training statistics\n",
    "        total_reward = sum(episode_rewards)\n",
    "        avg_reward = np.mean(episode_rewards) if episode_rewards else 0\n",
    "        final_exploration = metrics['exploration_rates'][-1] if metrics['exploration_rates'] else 0\n",
    "        \n",
    "        print(f\"\\\\n {name} Training Complete!\")\n",
    "        print(f\"     Training Time: {algorithm_duration:.1f} seconds\")\n",
    "        print(f\"    Total Reward: {total_reward:.2f}\")\n",
    "        print(f\"    Average Reward: {avg_reward:.3f}\")\n",
    "        print(f\"    Final Exploration: {final_exploration:.4f}\")\n",
    "        print(f\"    Model saved to: {experiment_instance.paths[name]}\")\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'episode_rewards': episode_rewards,\n",
    "            'training_time': algorithm_duration,\n",
    "            'total_reward': total_reward,\n",
    "            'average_reward': avg_reward,\n",
    "            'final_exploration': final_exploration,\n",
    "            'metrics': metrics,\n",
    "            'agent': agent,\n",
    "            'trainer': trainer\n",
    "        }\n",
    "    \n",
    "    total_training_time = time.time() - training_start_time\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*70)\n",
    "    print(\" TRAINING EXPERIMENT COMPLETED!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\" Total Experiment Time: {total_training_time:.1f} seconds\")\n",
    "    print(f\" Results Summary:\")\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        if result is not None:\n",
    "            print(f\"    {name}: {result['average_reward']:.3f} avg reward, \"\n",
    "                  f\"{result['training_time']:.1f}s training\")\n",
    "        else:\n",
    "            print(f\"    {name}: Training failed\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    return results\n",
    "\n",
    "# Execute the training experiment\n",
    "print(\" Executing training experiment...\")\n",
    "print(\"This will train both algorithms and track their learning progress.\")\n",
    "print(\" Grab a coffee - this may take several minutes!\\\\n\")\n",
    "\n",
    "# Run the experiment\n",
    "training_results = run_training_experiment(experiment_instance)\n",
    "\n",
    "print(\"\\\\n Training experiment completed successfully!\")\n",
    "print(\" Ready to analyze and visualize the results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0c7882",
   "metadata": {},
   "source": [
    "## 7. Analyze Training Results\n",
    "\n",
    "Now let's analyze and visualize the training results! We'll create comprehensive plots to understand:\n",
    "\n",
    "### Learning Curves Analysis\n",
    "- **Episode Rewards**: How reward accumulates over training episodes\n",
    "- **Convergence Speed**: Which algorithm learns faster\n",
    "- **Training Stability**: How consistent each algorithm's learning is\n",
    "- **Exploration Decay**: How exploration parameters change over time\n",
    "\n",
    "### Algorithm Comparison\n",
    "- **Final Performance**: Which algorithm achieved better final performance\n",
    "- **Learning Efficiency**: Reward gained per training time\n",
    "- **Exploration Strategy**: Differences in exploration vs exploitation balance\n",
    "- **Training Dynamics**: Patterns in learning behavior\n",
    "\n",
    "This analysis will help us understand the practical differences between traditional Q-Routing and Soft-Q-Routing in IAB networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa580e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive results visualization and comparison\n",
    "def visualize_experiment_results(experiment_results, agent_names):\n",
    "    \"\"\"Create comprehensive visualizations comparing the algorithms\"\"\"\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Training Progress Comparison\n",
    "    plt.subplot(3, 3, 1)\n",
    "    for i, name in enumerate(agent_names):\n",
    "        if experiment_results[name] is not None:\n",
    "            try:\n",
    "                train_stats = experiment_results[name]['train_stats']\n",
    "                # Plot training metrics if available\n",
    "                plt.plot(range(len(getattr(train_stats, 'rewards', []))), \n",
    "                        getattr(train_stats, 'rewards', []), \n",
    "                        label=name, color=colors[i], linewidth=2)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not plot training progress for {name}: {e}\")\n",
    "    \n",
    "    plt.title('Training Progress: Cumulative Rewards', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Training Episodes')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Convergence Analysis\n",
    "    plt.subplot(3, 3, 2)\n",
    "    for i, name in enumerate(agent_names):\n",
    "        if experiment_results[name] is not None:\n",
    "            try:\n",
    "                train_stats = experiment_results[name]['train_stats']\n",
    "                # Plot convergence metrics\n",
    "                losses = getattr(train_stats, 'losses', [])\n",
    "                if losses:\n",
    "                    plt.plot(losses, label=f'{name} Loss', color=colors[i], linewidth=2)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not plot convergence for {name}: {e}\")\n",
    "    \n",
    "    plt.title('Algorithm Convergence', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss/Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # 3. Test Performance Metrics\n",
    "    plt.subplot(3, 3, 3)\n",
    "    performance_data = {'Algorithm': [], 'Metric': [], 'Value': []}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IABenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
